---
layout: post
title: "Learning Deep Features for Discriminative Localization"
date: 2020-8-5
excerpt: ""
tags: [Object detection, image]
blog: false
feature: ./assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/cam.png 
comment: true
---


<style>
a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style>
<div class="page-body"><p id="910c47c3-1356-4124-b4ef-f794c162ca5a" class="">2015년에 나온 논문이라 Baseline과 trend가 지금과는 다소 다른 부분이 있다.</p><p id="0dabd868-15d3-45cd-b605-5cc16e63671d" class="">그럼에도 이번 논문을 리뷰하는 것은 Grad-CAM을 리뷰하기 위한 초석이며 해석가능한 딥러닝 모델을 공부함에 있어서 좋은 정보가 될 것이라 믿기 때문이다.</p><h1 id="71f85ac0-a87f-4cce-92b8-9b571a0cd646" class="">Introduction</h1><p id="aaa68316-b127-4ad1-ad73-5a2eab0d780f" class="">
</p><p id="0624db65-cbf9-4b4d-8a3c-4b1640cdf093" class="">Computer vision 에서 CNN이 다른 모델에 비해 압도적인 성능을 보이고 있다.  </p><p id="b5b3933d-526b-401b-9ef7-e0949f7c7160" class="">이는 Convolution layer이 그 구조상 localization에 뛰어난 능력을 갖기 때문이라고 설명된다.  </p><p id="e1bdc96f-81f1-4c75-afb2-3b49a75b621e" class="">
</p><p id="53527e42-4e74-4188-8b72-9f8a5301b6fe" class="">Conolution layer만을 이용해서는 모델의 output size를 조절하는 작업이 쉽지 않기 때문에 이미지 분류같은 구체적인 task를 하기 위해 Convolution layer를 거친 이후 Fully connected network (FCN)를 연결하여 사용하고 있다. </p><p id="a6c746c7-2411-47d5-8047-3ab859e021fc" class="">
</p><p id="c1692c29-120b-4120-8852-df4f130d957d" class="">문제는 여기서 발생하는데 FCN을 거치면서 Convolution layer의 localization 정보가 손실이 된다는 것이다.</p><p id="7fdadd29-68a3-4a3b-9006-1641bc4bbbe9" class="">
</p><p id="af3859f7-127d-46ad-89bf-3e396fe10906" class="">이번 논문에서는 Computer vision에서 일반적인 task (Object detection  or image classification) 모델의 localization 정보를 잃지 않으면서 task 성능은 거의 유사하게 유지하는 방법을 제안한다. 궁극적으로는 이런 localization 정보를 이용해 추가적인 task를 진행할 수 있음을 보인다.</p><p id="8632696e-3875-4875-94c2-7cf0044bc3f7" class="">
</p><p id="339d01ad-986a-4648-8706-bbb0059138b2" class="">
</p><h1 id="acaa4f31-bc1d-46b6-9ed4-358cfd3aaa9f" class="">Class activation mapping (CAM)</h1><p id="7076afa4-f31b-4b1d-88be-7ee1aeb5b1a7" class="">CAM은 특정 카테고리에 대해서 discriminative image region을 찾기 위한 도구이다. (Figure 3의 예시를 보자.)</p><figure id="ffd1c687-65ed-4459-9008-88d809ecdf7e" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled.png"><img style="width:550px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled.png"/></a></figure><p id="78774298-2f3e-44b1-ab60-a319dcad1ead" class="">
</p><p id="2dd192b8-9bee-47c3-9ca2-07f66ba65f81" class="">그렇다면 CAM은 어떤 방식으로 구현이 되는가?</p><p id="542f08b4-f1f9-4f7f-a1e2-9f98f5d010d6" class="">논문에서는  global average pooling을 이용해 CAM을 생성하는 모델을 제안한다.</p><p id="2ec6ea94-003d-447f-ac62-eeff68986e5c" class="">
</p><h3 id="ef7451fd-8f65-4466-afb9-ecdbefacb4ed" class=""><strong>global average pooling 개념</strong></h3><p id="ca8dbdaf-1e28-4991-bb19-1e948562bff7" class="">개별 feature map의 평균</p><figure id="dd45a80c-106c-4d47-b8ab-4776f992cd3d" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%201.png"><img style="width:384px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%201.png"/></a><figcaption><a href="https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/">https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/</a></figcaption></figure><hr id="e59b2fe5-5a50-40d1-8134-b552c27cb268"/><p id="b99e8ce6-0314-45fa-a18f-3a9af68459b8" class="">
</p><p id="2c9dd253-b171-4240-99e8-c3b5af64433e" class="">기존에는 global average pooling을 structural regularizer의 역할로 썼다면 이번 논문에서는 localization ability를 갖게 하는 도구로 사용한다.</p><p id="b461a087-6e54-473c-8d00-718c3db4c461" class="">
</p><p id="45598808-6750-4f61-9f96-9b87b3e29f4b" class="">
</p><h3 id="2b3b617a-6b95-4745-b710-107d9b97693a" class="">CAM을 생성하는 방법</h3><p id="c41ef197-ec51-4e06-8b99-8c7623c7f178" class="">CAM은 CNN을 사용하는 많은 Image classification model의 FCN 부분을 Global average pooling으로 대체함으로써 다양한 모델에서 쉽게 생성할 수 있다.</p><p id="1d9547b6-e11f-41fd-a249-05295cf2e183" class="">
</p><figure id="e8e8f61b-89c5-4295-9440-5258b241cbfc" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%202.png"><img style="width:939px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%202.png"/></a><figcaption>&lt;Global average pooling 분류 모델&gt;</figcaption></figure><p id="c0821a45-86b4-47f0-b17a-85b8c1c24378" class="">
</p><div id="333ddb85-8330-45c0-84d3-095e44f343e2" class="column-list"><div id="540f2ec7-06a3-4fc6-b770-3d0ca885ef80" style="width:31.25%" class="column"><figure id="6c15a62c-b36f-4bde-937e-9824ec1d9a91" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%203.png"><img style="width:175px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%203.png"/></a><figcaption>특정 label에 대한 output을 뽑기 위한 weighted sum</figcaption></figure></div><div id="26e70fbe-e1f5-46ef-b639-d3b498b5bacc" style="width:68.75%" class="column"><p id="0ba40afe-593c-4a90-b56f-02c657c9dfb3" class="">마지막 Convolution layer에서 나온 feature map을 global average pooling 한 이후 한 층의 FCN을 이용해 분류 작업을 진행한다. </p><p id="fd819eb5-e196-44a9-aac1-77bbb7fcce0d" class="">이후의 CAM을 만드는 원리에 대한 설명을 위해 조금 더 자세히 설명하자면</p><p id="9d401d61-c47d-4df6-8e42-aa44a55e9142" class="">Global average pooling을 이용해 나온 feature map의 평균값과 해당하는 class 사이의 weight을 곱해 합한 값이 해당 class에 대한 최종 output이된다. </p></div></div><p id="7a4388c1-0519-4e2f-b315-94631b8d06a9" class="">
</p><p id="15396328-49f6-4745-b540-9511dcc6ceba" class="">CAM을 생성하는 과정도 이와 유사하다.</p><ul id="9ea99441-ab0d-455f-8850-4bf0b93ee861" class="toggle"><li><details open=""><summary>논문에서 CAM을 설명하는 구체적인 수식은 아래와 같다. (궁금한 사람만 열어보자.)</summary><ol id="38685871-3d88-4fd3-b8d1-88f425b3edd0" class="numbered-list" start="1"><li><strong>global average pooling</strong></li></ol><figure id="398356a2-525a-4d49-bc7e-fb393b4e1dd5" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><p id="d4eb9397-2660-483f-b0ae-837acaee0bdb" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{k}(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> : k번째 feature map에서 x, y 좌표가 갖는 값</p><p id="7e13e74d-306a-4c1d-802a-b07230898d69" class="">
</p><p id="04c086bc-265a-4905-ab2f-b435dd0550b4" class="">map의 size가 동일하기 때문에 단순 합과 평균의 의미가 같기에 단순 합으로 표기한 것으로 보인다. </p><p id="55d4ff7b-04a3-446b-94fe-9c8fb091b6ed" class="">
</p><p id="a7e32f39-29cc-475c-a1eb-7f0cd8d2f5ca" class="">2. <strong>class score</strong></p><figure id="bf587bfa-2117-4a34-91d2-996e924e9d1d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext><mo>=</mo><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍  = ⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mord">⁍</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><p id="feaa00ea-8da5-4e29-a1ec-851511b6a269" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>w</mi><mi>k</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">w^c_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9474999999999999em;vertical-align:-0.2831079999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4168920000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> : class c에 해당하는 node의 weight 중 k번째 feature map과 상응하는 weight</p><p id="97ff8bd7-765b-449e-9003-2263d38d9bfd" class="">
</p><p id="876ecc54-cb7e-4447-99b0-fbf8b2f4358d" class="">
</p><p id="17c59277-0ba8-42b7-8d5c-635be6e16266" class="">3. <strong>importance of the activation at spatial grid (x, y)</strong></p><p id="7c26fe5f-faf2-4a1e-9742-ff2b99072679" class="">class score는 다음과 같이 표현할 수도 있다.</p><figure id="f68d17e4-78e4-4ba6-97db-3ab0e172891d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><p id="29e75437-9746-4208-a984-f7928b5800b4" class="">importance of the activation at spatial grid (x, y)를 다음과 같이 정의한다.</p><figure id="f220b681-f087-4c4a-b2f4-4486001f358a" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><p id="d11615a5-0e98-41e5-baf7-cec273eca799" class="">CAM의 가중합을 위한 weight을 결정하는 요인은 class와 feature map 뿐이다. </p><hr id="dd0bb5eb-bafe-434b-86c2-26040f5d28ff"/></details></li></ul><p id="9b6c7e7b-da53-405a-9721-00629ca3a472" class="">
</p><p id="bd0a0b41-298b-40c0-9309-e3906a99d2bb" class="">그림과 식 (토글을 봤다면)에서 알 수 있듯이 특정 Class에 대해서 feature map과 weight이 1대1로 대응하기 때문에 CAM의 구성은 다음 그림 한 장으로 설명이 가능하다.</p><p id="42fd57c4-3340-4927-bc48-f932503f5f6a" class="">
</p><figure id="29cf4d66-b215-48c0-ac92-39f64ee005fa" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%204.png"><img style="width:937px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%204.png"/></a></figure><p id="4de69853-f8ef-4a56-9176-60f462f9843f" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">W_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>은 &lt;Global average pooling 분류 모델&gt;의 weight과 동일하다.</p><p id="f42d2ce2-5707-4611-a438-6f5988e50eb9" class="">
</p><p id="45fed5d8-f7d4-4723-a1d2-460884852cd6" class="">
</p><h3 id="9340a4df-f969-4409-b0c4-b690ccd1b8db" class=""><strong>Global average pooling (GAP) vs Global max pooling (GMP)</strong></h3><p id="909bee49-6773-460b-b119-5e56b61c281c" class="">CAM을 구현하는 방법은 알게 되었지만 한 가지 궁금한 점이 있다.왜 Average를 사용하는 것인가? </p><p id="6b6d89ec-a01a-446d-850e-e774ec6dea36" class=""><strong>가장 중요한 이유는 GAP가 GMP보다 성능이 높다는 것이다.</strong></p><p id="a1f4ed8d-4711-4405-bf47-c9e84b873152" class="">
</p><p id="ed76dd59-359d-40e6-b860-f25453b93601" class="">deep learning model의 한계로 성능이 높은 이유를 구체적으로 설명을 할 수는 없다.</p><p id="cb87fe74-0501-4204-ad54-d5c41ebd8003" class="">논문에서는 Average가 discriminative part 전체를 고려할 수 있다면 Max는 discriminative boundary같이 극단적으로 변하는 부분을 크게 반영하기에 다른 세부적인 차이를 반영할 수 없다고 믿는다. (물론 믿음이다.)</p><p id="c2b3d2dc-111d-4b49-b964-bdbc5fa5d679" class="">
</p><p id="02e23f7c-917d-482d-8af1-48356ea36bef" class="">
</p><h1 id="35b3cab5-a246-42cf-b473-23a8f7e220ef" class="">Weakly-supervised Object Localization Result</h1><p id="a0659ec4-fc17-4766-acb1-2cda1da6f2fc" class="">이번 논문에서 localization을 위해서 bounding box가 정의된 데이터 셋을 이용하지 않는다.  (bounding box가 정의된 데이터라고 하더라도 bounding box 정보를 이용하지 않는다.) </p><p id="5dfb2882-3241-4b50-8be4-b6976026d65e" class="">
</p><p id="a5d6851f-4332-4878-b754-d11f9dccc6fd" class="">imagenet dataset과 같이 bounding box가 구비된 데이터 셋은 현실에서 매우 적기 때문에 bounding box를 이용해서 모델을 학습하는 것은 현실에서 매우 어렵다.</p><p id="4463db9f-f8f5-4781-875d-85346284a0f4" class="">
</p><p id="fc617fe4-447c-422b-9976-b0e0ed456a54" class="">따라서, image의  label을 분류하는 모델을 학습하는 것만으로도 Object localization 기능을 추가적으로 하는 weakly-supervised object localization 방법을 이용한다.</p><p id="39db2aed-074f-4b3c-a5c5-754478e6e941" class="">
</p><h3 id="442cce1a-659b-4865-9e75-6aaed09728dd" class="">setup</h3><p id="c433dd7d-6add-47cd-a8c9-f40cd70454d8" class="">2015년 당시 유명했던 모델의 FCN 부분을 GAP로 바꿈으로써 CAM을 구현한다. 사용한 모델과 baseline 모델은 다음과 같다.</p><figure id="66566cb8-db07-466a-a49e-0b14c66c4e77" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><p id="d1b210d5-aca4-402c-9a22-24e67e13b922" class="">
</p><h3 id="9c316143-5c6d-4cc8-8dd6-632cf725d16e" class="">Classification &amp; Localization performance</h3><p id="8d2016c4-24bd-4357-a19f-94a7c8d82f6b" class="">
</p><p id="b077523b-d45f-4756-84bd-69385c69e35e" class=""><strong>Classification</strong></p><figure id="e6c58b4b-01e2-4f03-ad14-02f3f6afa68d" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%205.png"><img style="width:407px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%205.png"/></a></figure><p id="d6378506-b2a8-4d2f-b5db-0d075d9d3054" class="">classification의 성능은 약간의 감소를 보인다.</p><p id="3d86fb02-409a-4c18-b619-4b0409046d10" class="">
</p><p id="bf28383c-4849-4bdf-bfd8-5ef74133f52b" class=""><strong>weakly-supervised Localization baseline</strong></p><figure id="9a47c817-4be6-40c0-85d0-270734b1be46" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%206.png"><img style="width:461px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%206.png"/></a></figure><p id="36415908-ba60-40bb-afa6-5620b4baabaf" class="">weakly-supervised localization baseline 모델보다 더 우수한 성능을 보인다.</p><p id="a5422187-e350-4757-b843-eef960690c52" class="">실제 예시를 통해 localization의 activation map을 비교해보면 GAP을 이용한 모델이 좀 더 label의 의미에 가까운 부분을 포착하는 것을 확인할 수 있다.</p><p id="24cc7de3-f59b-44d9-987c-4e7941d69a4b" class="">
</p><figure id="230b3cd6-cb12-45fb-a3d3-cec852105c61" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%207.png"><img style="width:960px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%207.png"/></a></figure><p id="9b5d8f89-0b4f-4088-a046-574f226ecab2" class="">
</p><figure id="3e6af5da-1b77-4f16-8eb2-f9396d836731" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%208.png"><img style="width:957px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%208.png"/></a></figure><p id="8bfa3285-f980-4294-aaff-394a61e8518c" class="">
</p><p id="e49d5a3b-1df1-4079-a1a3-6e59e67e7e01" class=""><strong>fully-supervised localization baseline</strong></p><figure id="7c65c5a7-9e91-45ab-9231-ad547871c212" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%209.png"><img style="width:458px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%209.png"/></a></figure><p id="d27347e3-d5ef-4049-8e9e-a381d3f7e103" class="">fully-supervised 모델에 비하면 성능이 다소 떨어지긴 하지만 AlexNet과 거의 유사한 수준의 성능이 나옴을 확인할 수 있다. </p><p id="b366399a-4b94-41fa-80d5-4187c55945a8" class="">
</p><h1 id="42cabafa-2a34-4416-908d-d05b917c1745" class="">Deep Features for Genenric Localization</h1><p id="29e88e52-5fc0-463b-aadf-54e7903c7210" class="">아래의 그림은 CAM을 통해 얻은 Deep feature의 generic localization 성능이  우수함을 보이는 예시이다. </p><figure id="6d116514-f2e3-48f3-8d1b-abdb176761ce" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2010.png"><img style="width:956px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2010.png"/></a></figure><p id="eb1cc4bb-bdeb-45e3-a560-0e0c191540ee" class="">
</p><p id="8a0c1a8a-ff71-455c-a85b-cace675c883a" class="">
</p><p id="64e1387b-8ebc-49aa-9c61-94511853b3dd" class="">논문에서는 CAM을 이용해서 다양한 문제들을 풀었다. 아래는 그것들의 예시이다.</p><h3 id="7bf4902d-250e-4aa6-b706-b0edc53c50e9" class="">Discovering informative objects in the scenes</h3><figure id="d2453b18-5499-45f5-b94d-3869d969aa27" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2011.png"><img style="width:463px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2011.png"/></a></figure><p id="32a4b6ec-2188-4709-a417-43abf8b48edd" class="">
</p><h3 id="350c2091-ecb4-4343-8c73-9a8431d6173a" class="">Concept localization in weakly labeled images</h3><figure id="46bc5270-9376-4d23-8ced-296c1244a3a7" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2012.png"><img style="width:465px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2012.png"/></a></figure><p id="8198dcd6-6c82-4ac2-9b10-66dd2ac50585" class="">
</p><h3 id="7feeebc0-0f30-4852-850f-16b9ce3fd56a" class="">Weakly supervised text detector</h3><figure id="1e2ca2a2-bc50-44c7-84e9-25a2046b63a5" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2013.png"><img style="width:456px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2013.png"/></a></figure><p id="5e194337-5ac1-41c3-a80a-f9416795f0ef" class="">
</p><h3 id="04364fcf-114e-4e38-852a-67fc7e5df4f9" class="">Interpreting visual question answering</h3><figure id="967bb830-5e16-4d78-a9c5-01540eb154dc" class="image"><a href="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2014.png"><img style="width:464px" src="../assets/img/Learning%20Deep%20Features%20for%20Discriminative%20Localiza%20ffd1c68765ed4459900888d809ecdf7e/Untitled%2014.png"/></a></figure><p id="c729a9ab-5612-4a93-9718-adbe76dc210a" class="">
</p><p id="ee125499-c561-4b33-9dd8-aa5d1b7d5d88" class="">
</p><h1 id="ebe9ebb6-1bba-4410-a01f-d98549333fee" class="">Reference</h1><ul id="ff8d5f53-86ac-48eb-8a24-f7d94f11408e" class="bulleted-list"><li><a href="https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/">https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/</a></li></ul><ul id="975416d9-f03c-4159-84cf-6510dfdd83f3" class="bulleted-list"><li><a href="http://tmmse.xyz/2016/04/10/object-localization-with-weakly-supervised-learning/">http://tmmse.xyz/2016/04/10/object-localization-with-weakly-supervised-learning/</a></li></ul></div>